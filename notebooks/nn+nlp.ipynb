{"metadata": {"language_info": {"mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "file_extension": ".py", "version": "3.6.3"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat": 4, "nbformat_minor": 1, "cells": [{"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np\n", "import os\n", "import pandas as pd\n", "import sys\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.feature_extraction.text import TfidfTransformer\n", "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer\n", "from sklearn.naive_bayes import MultinomialNB\n", "from sklearn.svm import LinearSVC\n", "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n", "from sklearn.linear_model import LogisticRegression,SGDClassifier\n", "from nltk.corpus import wordnet as wn\n", "from nltk.corpus import stopwords\n", "from nltk.stem.snowball import SnowballStemmer\n", "from nltk.stem import PorterStemmer\n", "import nltk\n", "from nltk import word_tokenize, ngrams\n", "from nltk.classify import SklearnClassifier\n", "from wordcloud import WordCloud,STOPWORDS\n", "import xgboost as xgb\n", "np.random.seed(25)\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "train = pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv(\"../input/test.csv\")\n", "# Any results you write to the current directory are saved as output."], "metadata": {"_uuid": "2ed1ca776494fcf1a1a3b3e15dc45a0028084612", "_cell_guid": "2cb8c4af-1f0f-45c5-9835-bcaa0884fc60"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["train.head()"], "metadata": {"_uuid": "e748224a9020db4fb4937dbff66cc52af8b9b960", "_cell_guid": "0fe92c4c-4dcc-4a05-af1e-0c7da467736d"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Target Mapping\n", "mapping_target = {'EAP':0, 'HPL':1, 'MWS':2}\n", "train = train.replace({'author':mapping_target})"], "metadata": {"_uuid": "5fd18f20b2a2132e51005616c059b2fbe83611d0", "collapsed": true, "_cell_guid": "6de5dde1-0376-41f2-a77e-f5a06db6390e"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["train.head()"], "metadata": {"_uuid": "e9ea83988b68cedb52ac3a0cbeb5a583c27db8a8", "_cell_guid": "f1656e9e-578a-4fd2-93c7-6bd88d5f9f5e"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["test_id = test['id']\n", "target = train['author']"], "metadata": {"_uuid": "b16cfbca25e8311963c7ef2496701678720c9fb1", "collapsed": true, "_cell_guid": "597d190a-8c7e-4beb-a909-0e19fc9ccab8"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# function to clean data\n", "import string\n", "import itertools \n", "import re\n", "from nltk.stem import WordNetLemmatizer\n", "from string import punctuation\n", "\n", "def preprocess(text):\n", "    text = text.strip()\n", "    text = text.replace(\"' \", \" ' \")\n", "    signs = set(',.:;\"?!')\n", "    prods = set(text) & signs\n", "    if not prods:\n", "        return text\n", "\n", "    for sign in prods:\n", "        text = text.replace(sign, ' {} '.format(sign) )\n", "    return text\n", "\n", "def cleanData(text, lowercase = False, remove_stops = False, stemming = False, lemmatization = False):\n", "    \n", "    txt = str(text)\n", "    \n", "    txt = re.sub(r'[^A-Za-z\\s]',r' ',txt)\n", "    \n", "     \n", "    if lowercase:\n", "        txt = \" \".join([w.lower() for w in txt.split()])\n", "        \n", "    if remove_stops:\n", "        txt = \" \".join([w for w in txt.split() if w not in stops])\n", "    if stemming:\n", "        st = PorterStemmer()\n", "        txt = \" \".join([st.stem(w) for w in txt.split()])\n", "    \n", "    if lemmatization:\n", "        wordnet_lemmatizer = WordNetLemmatizer()\n", "        txt = \" \".join([wordnet_lemmatizer.lemmatize(w, pos='v') for w in txt.split()])\n", "\n", "    return txt"], "metadata": {"_uuid": "8c987eff81376f41ffdb8e4e4e53e208e8663460", "collapsed": true, "_cell_guid": "9b40c219-8b32-4a2d-8c38-1bc4691c71ab"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["train['text'] = train['text'].map(lambda x: preprocess(x))\n", "test['text'] = test['text'].map(lambda x: preprocess(x))"], "metadata": {"_uuid": "0196ba72a8cf5a3e2d95b8252143b5e20b57ce6f", "collapsed": true, "_cell_guid": "628ddc08-8e61-407f-8c73-617e1a39e5e2"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# clean text\n", "train['text'] = train['text'].map(lambda x: cleanData(x, lowercase=True, remove_stops=False, stemming=False, lemmatization = False))\n", "test['text'] = test['text'].map(lambda x: cleanData(x, lowercase=True, remove_stops=False, stemming=False, lemmatization = False))"], "metadata": {"_uuid": "d4b0e8b202f9954a52082e1011907f8f12b3eb9b", "collapsed": true, "_cell_guid": "161467ed-5748-4b0b-b2f8-aeba6221e283"}}, {"cell_type": "markdown", "source": ["# Using Neural Network"], "metadata": {"_uuid": "23d266451f858f76cf82d070c2914961e75229ac", "collapsed": true, "_cell_guid": "e4d0ceef-3a1f-40f4-ba47-857c3f5c81e5"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["np.random.seed(25)\n", "from keras.models import Sequential\n", "from keras.preprocessing.text import Tokenizer\n", "from keras.preprocessing.sequence import pad_sequences\n", "from keras.utils.np_utils import to_categorical\n", "from keras.layers import Dense, Input, Flatten, merge, LSTM, Lambda, Dropout\n", "from keras.layers import Conv1D, MaxPooling1D, Embedding\n", "from keras.models import Model\n", "from keras.layers.wrappers import TimeDistributed, Bidirectional\n", "from keras.layers.normalization import BatchNormalization\n", "from keras import backend as K\n", "from keras.layers import Convolution1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n", "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n", "from keras.layers.merge import concatenate\n", "from keras.layers.core import Dense, Activation, Dropout\n", "import codecs"], "metadata": {"_uuid": "6811b5188a429fca473f1006bc5e06e384583777", "_cell_guid": "5168bc65-7ee1-49c9-a4f4-1d50032e337b"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["MAX_SEQUENCE_LENGTH = 256\n", "MAX_NB_WORDS = 200000"], "metadata": {"_uuid": "f04bb2ae8618b5b4d5ad7cd7ee12bc1f640cf962", "collapsed": true, "_cell_guid": "61aea7ce-e90c-4d07-b2ac-24ed5cc8e0dc"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["def add_ngram(q, n_gram_max):\n", "            ngrams = []\n", "            for n in range(2, n_gram_max+1):\n", "                for w_index in range(len(q)-n+1):\n", "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n", "            return q + ngrams"], "metadata": {"_uuid": "7dc2c4bbd4edb1c210389c70fd9ec84cd133d22b", "collapsed": true, "_cell_guid": "3defc665-523f-44e7-80ea-a7cdfeeee706"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["n_gram_max = 2\n", "print('Processing text dataset')\n", "texts_1 = []\n", "for text in train['text']:\n", "    text = text.split()\n", "    texts_1.append(' '.join(add_ngram(text, n_gram_max)))\n", "    \n", "\n", "labels = train['author']  # list of label ids\n", "\n", "print('Found %s texts.' % len(texts_1))\n", "test_texts_1 = []\n", "for text in test['text']:\n", "    text = text.split()\n", "    test_texts_1.append(' '.join(add_ngram(text, n_gram_max)))\n", "print('Found %s texts.' % len(test_texts_1))"], "metadata": {"_uuid": "256cb736ba230aaa9a81d8eb0c9388204ab7b4f3", "_cell_guid": "1877b98d-c7e1-4d1e-8f02-5685154ab261"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["min_count = 2\n", "tokenizer = Tokenizer(lower=False, filters='')\n", "tokenizer.fit_on_texts(texts_1)\n", "num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n", "\n", "tokenizer = Tokenizer(num_words=num_words, lower=True, filters='')\n", "tokenizer.fit_on_texts(texts_1)\n", "\n", "\n", "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n", "# word_index = tokenizer.word_index\n", "# print('Found %s unique tokens.' % len(word_index))\n", "\n", "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n", "\n", "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n", "labels = np.array(labels)\n", "print('Shape of data tensor:', data_1.shape)\n", "print('Shape of label tensor:', labels.shape)\n", "\n", "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n", "#test_labels = np.array(test_labels)\n", "del test_sequences_1\n", "del sequences_1\n", "import gc\n", "gc.collect()"], "metadata": {"_uuid": "9e5a87606694be4317c8dfabea0367bc30f40626", "_cell_guid": "97dfbeef-98e4-44c6-a3f5-4b4944b2fd17"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["nb_words = np.max(data_1) + 1 #min(MAX_NB_WORDS, len(word_index)) + 1"], "metadata": {"_uuid": "1994e200be8a3f3c3aa2e3f5fda60aaf8f40ccef", "collapsed": true, "_cell_guid": "6a36b388-fc58-4f5e-be9b-5d0aed587254"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["nb_words"], "metadata": {"_uuid": "5649820ad79c83da2301ab97d426b9c734f99769", "_cell_guid": "096dcf66-890a-499c-b4b8-fb67ab61160e"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["from keras.layers.recurrent import LSTM, GRU\n", "model = Sequential()\n", "model.add(Embedding(nb_words,20,input_length=MAX_SEQUENCE_LENGTH))\n", "# model.add(Flatten())\n", "# model.add(Dense(100, activation='relu'))\n", "# model.add(Dropout(0.3))\n", "# model.add(Conv1D(64,\n", "#                  5,\n", "#                  padding='valid',\n", "#                  activation='relu'))\n", "# model.add(Dropout(0.3))\n", "model.add(GlobalAveragePooling1D())\n", "# model.add(Flatten())\n", "# model.add(Dense(100, activation='relu'))\n", "# model.add(Dropout(0.5))\n", "model.add(Dense(3, activation='softmax'))\n", "\n", "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])"], "metadata": {"_uuid": "edab633734e3f18ee93738d90d3f50538caba730", "collapsed": true, "_cell_guid": "5fc2074d-c97d-42c7-9520-287a82a46dbb"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["model.fit(data_1, to_categorical(labels), validation_split=0.2, nb_epoch=15, batch_size=16)"], "metadata": {"_uuid": "a18313a64627ac15ffcd8491028a65f2abd072f4", "_cell_guid": "eb081f32-9b55-40ed-ae98-ecb3bdd6fa0c"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["preds = model.predict(test_data_1)"], "metadata": {"_uuid": "656bf99e26d40ae97d32b4b78841bd9709369a90", "collapsed": true, "_cell_guid": "8fb234fc-3415-4f6d-8a31-a7a2523ff739"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["result = pd.DataFrame()\n", "result['id'] = test_id\n", "result['EAP'] = [x[0] for x in preds]\n", "result['HPL'] = [x[1] for x in preds]\n", "result['MWS'] = [x[2] for x in preds]\n", "\n", "result.to_csv(\"result.csv\", index=False)"], "metadata": {"_uuid": "dbc9c5a25da1e427b3e0a863b686a5c0a4305ef1", "collapsed": true, "_cell_guid": "179acab2-3204-4b38-81e8-b6d4d325d351"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["result.head()"], "metadata": {"_uuid": "69d807a1bab626c5f7da9a438706dc374c41a902", "_cell_guid": "7ec1d3f0-52da-4bae-a5d1-7315fa2984f5"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": [], "metadata": {"_uuid": "bc1a7b899fb8df036071d66aa8e3cf1fdab0b096", "collapsed": true, "_cell_guid": "4547b418-1226-4255-8c4d-0383cce52123"}}]}