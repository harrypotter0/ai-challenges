{"metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python", "file_extension": ".py", "nbconvert_exporter": "python", "version": "3.6.1", "mimetype": "text/x-python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 1, "cells": [{"source": ["# Explaining the NLP terms   <br>\n", "<div class=\"section\" id=\"the-bag-of-words-representation\">\n", "<h3>1 The Bag of Words representation<a class=\"headerlink\" href=\"#the-bag-of-words-representation\" title=\"Permalink to this headline\">\u00b6</a></h3>\n", "<p>Text Analysis is a major application field for machine learning\n", "algorithms. However the raw data, a sequence of symbols cannot be fed\n", "directly to the algorithms themselves as most of them expect numerical\n", "feature vectors with a fixed size rather than the raw text documents\n", "with variable length.</p>\n", "<p>In order to address this, scikit-learn provides utilities for the most\n", "common ways to extract numerical features from text content, namely:</p>\n", "<ul class=\"simple\">\n", "<li><strong>tokenizing</strong> strings and giving an integer id for each possible token,\n", "for instance by using white-spaces and punctuation as token separators.</li>\n", "<li><strong>counting</strong> the occurrences of tokens in each document.</li>\n", "<li><strong>normalizing</strong> and weighting with diminishing importance tokens that\n", "occur in the majority of samples / documents.</li>\n", "</ul>\n", "<p>In this scheme, features and samples are defined as follows:</p>\n", "<ul class=\"simple\">\n", "<li>each <strong>individual token occurrence frequency</strong> (normalized or not)\n", "is treated as a <strong>feature</strong>.</li>\n", "<li>the vector of all the token frequencies for a given <strong>document</strong> is\n", "considered a multivariate <strong>sample</strong>.</li>\n", "</ul>\n", "<p>A corpus of documents can thus be represented by a matrix with one row\n", "per document and one column per token (e.g. word) occurring in the corpus.</p>\n", "<p>We call <strong>vectorization</strong> the general process of turning a collection\n", "of text documents into numerical feature vectors. This specific strategy\n", "(tokenization, counting and normalization) is called the <strong>Bag of Words</strong>\n", "or \u201cBag of n-grams\u201d representation. Documents are described by word\n", "occurrences while completely ignoring the relative position information\n", "of the words in the document.</p>\n", "</div>\n", "<p><a class=\"reference internal\" href=\"generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer\" title=\"sklearn.feature_extraction.text.CountVectorizer\"><code class=\"xref py py-class docutils literal\"><span class=\"pre\">CountVectorizer</span></code></a> implements both tokenization and occurrence\n", "counting in a single class:</p>"], "metadata": {"_cell_guid": "98c5c304-c58c-4f97-947c-49a98a187af3", "_uuid": "f4032b042154661728b4b2d7851a21530a66e621"}, "cell_type": "markdown"}, {"metadata": {"_cell_guid": "d19d5e90-a77a-48a4-b789-e78753e369cd", "collapsed": true, "_uuid": "0eb16b0a1e4b74ccad01f47370bf16e6ba5d0baa"}, "outputs": [], "execution_count": null, "source": ["from sklearn.feature_extraction.text import CountVectorizer"], "cell_type": "code"}, {"metadata": {"_cell_guid": "ba40f73f-7095-4df4-af77-a8bdc5294fe6", "_uuid": "3263da18041071f7b324e3c9a88f1feae5720be0"}, "outputs": [], "execution_count": null, "source": ["vect = CountVectorizer()\n", "vect"], "cell_type": "code"}, {"source": ["<br><p>Let\u2019s use it to tokenize and count the word occurrences of a minimalistic\n", "corpus of text documents:</p>"], "metadata": {"_cell_guid": "27152481-d49f-4dcf-ba59-a1b666744279", "_uuid": "99d1a5edc81162dd53e3cf404e613ae92d5a657d"}, "cell_type": "markdown"}, {"metadata": {"_cell_guid": "b069d6f1-3a7d-4244-8554-b06a85d25489", "_uuid": "9ee0592c12ef4bafbd886adc208ab2c07d6f1380"}, "outputs": [], "execution_count": null, "source": ["corpus = ['Hi my name is kanav.','I love reading.','Kanav loves reading scripts.']\n", "X= vect.fit_transform(corpus)\n", "X # note the dimensions of X(3X9) means 3 rows and 9 columns. "], "cell_type": "code"}, {"source": ["<br>Note the dimensions of X (3X9) means 3 rows and 9 columns. <br>\n", "as there are three documents and 9 unique words<br>\n", "See"], "metadata": {"_cell_guid": "56b04008-af4a-4693-ac87-9082013513a6", "_uuid": "9be3b64dbd7d47f7180b50567b42af6628589b8e"}, "cell_type": "markdown"}, {"metadata": {"_cell_guid": "7c1a646b-a698-49ac-ba03-776daf18b68f", "_uuid": "aa0e66b552c7f11339caa93baa36caf5084714e4"}, "outputs": [], "execution_count": null, "source": ["vect.get_feature_names()"], "cell_type": "code"}, {"source": ["### See this is the frequency matrix in the given documents"], "metadata": {"_cell_guid": "b95ed7a8-2f13-49b5-8ef0-94fbc6deb50d", "_uuid": "44c97e6e6db143493d2542ad296f5cddba6e3b05"}, "cell_type": "markdown"}, {"source": ["Each term found by the analyzer during the fit is assigned a unique integer index corresponding to a column in the resulting matrix. This interpretation of the columns can be retrieved as follows:"], "metadata": {"_cell_guid": "5cbcfa14-baee-4a46-af66-011bc24c6a01", "_uuid": "bab67cdd065aa5dc7b28da8cee62b823140d771d"}, "cell_type": "markdown"}, {"metadata": {"_cell_guid": "c25ac4c3-2283-4822-b01c-98634f216e8e", "_uuid": "5a3fa876b5b4770d411c5870e982d384fd763a62"}, "outputs": [], "execution_count": null, "source": ["X.toarray()"], "cell_type": "code"}, {"source": ["Hence words that were not seen in the training corpus will be completely ignored in future calls to the transform method:"], "metadata": {"_cell_guid": "4be526e6-c7b7-45e0-9206-03916234fb68", "_uuid": "faffe49878430e8b73ce2b58b70976d3a9410ddf"}, "cell_type": "markdown"}, {"metadata": {"_cell_guid": "022e678a-bd1a-45ce-abbc-6c1ff7c2d945", "_uuid": "a229b172df59e679eedb8f91b4176cc8f7bbba94"}, "outputs": [], "execution_count": null, "source": ["vect.transform(['hi,whats your name?.']).toarray()"], "cell_type": "code"}, {"source": ["### Normalization and stemming\n", "Since the words like love and loves has same meaning so, why not we treat them same?\n"], "metadata": {"_cell_guid": "7606035a-e08d-42b3-adfe-690c3b72f248", "_uuid": "ab3c29d7c170928c5f2bb2b6d7d7949358fd5cc6"}, "cell_type": "markdown"}, {"metadata": {"_cell_guid": "cf31de02-63be-459c-b23a-dd316c1e5fa8", "_uuid": "4acdb60ec8ab8761e1a2f2f9879f1b0c8c003533"}, "outputs": [], "execution_count": null, "source": ["import nltk\n", "porter = nltk.PorterStemmer()\n", "[porter.stem(t) for t in vect.get_feature_names()]"], "cell_type": "code"}, {"source": ["See the loves has now become love."], "metadata": {"_cell_guid": "95f4f41d-4f0d-4ddc-a013-50f17a825550", "_uuid": "c5cfdc86f340649b0f1515c43fe41445e0a53ea5"}, "cell_type": "markdown"}, {"source": ["Now we have total 8 unique features"], "metadata": {"_cell_guid": "bb7df4d3-389f-4f38-b64a-010eac28da36", "_uuid": "b130e487e31662ad3bbda2957c5b771015a5f854"}, "cell_type": "markdown"}, {"metadata": {"_cell_guid": "331447f9-bf8a-4565-9dd0-3ca290eb80ee", "_uuid": "59ec85c98e287e17031689b12155a736097f549c"}, "outputs": [], "execution_count": null, "source": ["list(set([porter.stem(t) for t in vect.get_feature_names()]))"], "cell_type": "code"}, {"metadata": {"_cell_guid": "46c00b36-7a39-4836-87c1-c6c12e0d4112", "_uuid": "cf1dbd710652ff529bfc380667bf3fdcd371bea3"}, "outputs": [], "execution_count": null, "source": ["WNlemma = nltk.WordNetLemmatizer()\n", "[WNlemma.lemmatize(t) for t in list(set([porter.stem(t) for t in vect.get_feature_names()]))]"], "cell_type": "code"}, {"source": ["# Lemmatization\n", "A very similar operation to stemming is called lemmatizing. The major difference between these is, as you saw earlier, stemming can often create non-existent words, whereas lemmas are actual words.\n", "\n", "So, your root stem, meaning the word you end up with, is not something you can just look up in a dictionary, but you can look up a lemma.\n", "\n", "Some times you will wind up with a very similar word, but sometimes, you will wind up with a completely different word. Let's see some examples."], "metadata": {"_cell_guid": "a8102f0f-ce68-4584-a578-3a9f24a47678", "_uuid": "0dcfd364a150043cd6890471fc40cd6c99573949"}, "cell_type": "markdown"}, {"metadata": {"_cell_guid": "ecad571a-f85c-4db4-a115-f2f35c31c45d", "_uuid": "9622010ce3f85837073aa66d39ffd9c59bf682b6"}, "outputs": [], "execution_count": null, "source": ["from nltk.stem import WordNetLemmatizer\n", "\n", "lemmatizer = WordNetLemmatizer()\n", "print(lemmatizer.lemmatize(\"cats\"))\n", "print(lemmatizer.lemmatize(\"cacti\"))\n", "print(lemmatizer.lemmatize(\"geese\"))\n", "print(lemmatizer.lemmatize(\"rocks\"))\n", "print(lemmatizer.lemmatize(\"python\"))\n", "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n", "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n", "print(lemmatizer.lemmatize(\"run\"))\n", "print(lemmatizer.lemmatize(\"run\",'v'))"], "cell_type": "code"}, {"source": ["## Please upvote!\n", "i will be keep on updating !"], "metadata": {"_cell_guid": "5c88128e-e6ed-405f-99b8-6dc253c09b32", "collapsed": true, "_uuid": "243269f3e4cc6fdf02f2995b682aba74e1eb7ba3"}, "cell_type": "markdown"}]}