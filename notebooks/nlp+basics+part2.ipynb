{"cells":[{"metadata":{"collapsed":true,"_cell_guid":"fc23fbbe-af8e-46bc-96fe-975e5f086555","_uuid":"9c02b71d9bf28e1b69f1ddfde3de90da7800a370"},"cell_type":"markdown","source":"This kernel is an extension of the EDA notebook: [Stop the S@#$ - Toxic Comments EDA](https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda)\n\n\n# Topic Modeling: \n\nTopic modeling can be a useful tool to summarize the context of a huge corpus(text) by guessing what the \"Topic\" or the general theme of the sentence. \n\nThis can also be used as inputs to our classifier if they can identify patterns or \"Topics\" that indicate toxicity.\n\nLet's find out!\n\nThe steps followed in this kernel:\n* Preprocessing (Tokenization using gensim's simple_preprocess)\n* Cleaning\n    * Stop word removal\n    * Bigram collation\n    * Lemmatization\n*  Creation of dictionary (list of all words in the cleaned text)\n* Topic modeling using LDA\n* Visualization with pyLDAviz\n* Convert topics to sparse vectors\n* Feed sparse vectors to the model"},{"metadata":{"collapsed":true,"_cell_guid":"56a4f2a9-4722-48de-b13e-78ac420b882c","_uuid":"af6903727c9870cae787456c67a32a2c743e2965","trusted":false},"cell_type":"code","source":"#import required packages\n#basic\nimport pandas as pd \nimport numpy as np\n\n#misc\nimport gc\nimport time\nimport warnings\n#viz\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec \nimport seaborn as sns\nimport pyLDAvis.gensim\n#nlp\nimport string\nimport re     #for regex\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nimport gensim\nfrom gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\nfrom gensim.models.wrappers import LdaMallet\nfrom gensim.corpora import Dictionary\n\n\n#Modeling\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_is_fitted\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom scipy import sparse\n\n#settings\nstart_time=time.time()\ncolor = sns.color_palette()\nsns.set_style(\"dark\")\n\n#constants\neng_stopwords = set(stopwords.words(\"english\"))\n#settings\nwarnings.filterwarnings(\"ignore\")\nlem = WordNetLemmatizer()\ntokenizer=ToktokTokenizer()\n\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"6801c92e-8193-4081-a614-4e40a517c192","_uuid":"fac352d68e9a649d13026d1bb38338ca60aafbe3","trusted":false},"cell_type":"code","source":"start_time=time.time()\n#importing the dataset\ntrain=pd.read_csv(\"../input/train.csv\")\ntest=pd.read_csv(\"../input/test.csv\")\nend_import=time.time()\nprint(\"Time till import:\",end_import-start_time,\"s\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"f440402e-ae9d-48a3-a335-54d85ff382ee","_uuid":"cce42167a934c2e0179d10f265e736eae131a442","trusted":false},"cell_type":"code","source":"#to seperate sentenses into words\ndef preprocess(comment):\n    \"\"\"\n    Function to build tokenized texts from input comment\n    \"\"\"\n    return gensim.utils.simple_preprocess(comment, deacc=True, min_len=3)\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"ad50f214-d3f8-4644-9343-7be675e61c8e","_uuid":"a17a336561b1046656b16709cddec53223d42dce","trusted":false},"cell_type":"code","source":"#tokenize the comments\ntrain_text=train.comment_text.apply(lambda x: preprocess(x))\ntest_text=test.comment_text.apply(lambda x: preprocess(x))\nall_text=train_text.append(test_text)\nend_preprocess=time.time()\nprint(\"Time till pre-process:\",end_preprocess-start_time,\"s\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"f017c456-875d-4be7-9dac-aab5c29785a7","_uuid":"a9f932049e841be09cbbbaae1a71c1c6aac7d0a7","trusted":false},"cell_type":"code","source":"#checks\nprint(\"Total number of comments:\",len(all_text))\nprint(\"Before preprocessing:\",train.comment_text.iloc[30])\nprint(\"After preprocessing:\",all_text.iloc[30])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"2ac48533-48f6-464e-b0ab-652dfb746502","_uuid":"c09e82a74704b8c2cad2f40ee7a377a253bec901","trusted":false},"cell_type":"code","source":"#Phrases help us group together bigrams :  new + york --> new_york\nbigram = gensim.models.Phrases(all_text)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"03b985dc-8621-4d6b-9070-5eb0f82c412d","_uuid":"08218ef8aab73ba9a1d7b277774ceb7579d9a8eb","trusted":false},"cell_type":"code","source":"#check bigram collation functionality \nbigram[all_text.iloc[30]]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"edf91081-9236-4d49-9793-24aee85c2814","_uuid":"6dadbbebf01c8c35ac683fc915bf74960e450c96","trusted":false},"cell_type":"code","source":"def clean(word_list):\n    \"\"\"\n    Function to clean the pre-processed word lists \n    \n    Following transformations will be done\n    1) Stop words removal from the nltk stopword list\n    2) Bigram collation (Finding common bigrams and grouping them together using gensim.models.phrases)\n    3) Lemmatization (Converting word to its root form : babies --> baby ; children --> child)\n    \"\"\"\n    #remove stop words\n    clean_words = [w for w in word_list if not w in eng_stopwords]\n    #collect bigrams\n    clean_words = bigram[clean_words]\n    #Lemmatize\n    clean_words=[lem.lemmatize(word, \"v\") for word in clean_words]\n    return(clean_words)    ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"33ae1471-5062-4e79-953d-f5798f929054","_uuid":"fb1344f5619e410ac4a5291e70a179184f6c5f54","trusted":false},"cell_type":"code","source":"#check clean function\nprint(\"Before clean:\",all_text.iloc[1])\nprint(\"After clean:\",clean(all_text.iloc[1]))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"b4fc4117-f0e4-4e42-98c1-5a52eb45eb72","_uuid":"83bbb2f9501083f4c8666ea01a9f306202b8d53a","trusted":false},"cell_type":"code","source":"#scale it to all text\nall_text=all_text.apply(lambda x:clean(x))\nend_clean=time.time()\nprint(\"Time till cleaning corpus:\",end_clean-start_time,\"s\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"f90f930d-c1b8-45b5-87f7-95bd2b416af8","_uuid":"1d884c25d60f0edc61dd3e2080c6f972acf65d18","trusted":false},"cell_type":"code","source":"#create the dictionary\ndictionary = Dictionary(all_text)\nprint(\"There are\",len(dictionary),\"number of words in the final dictionary\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"880b0438-b0bd-4b02-b394-a91bb1c3f76a","_uuid":"2a49836593a098ad870e8a5154f6db0492f8080c","trusted":false},"cell_type":"code","source":"#convert into lookup tuples within the dictionary using doc2bow\nprint(dictionary.doc2bow(all_text.iloc[1]))\nprint(\"Wordlist from the sentence:\",all_text.iloc[1])\n#to check\nprint(\"Wordlist from the dictionary lookup:\", \n      dictionary[21],dictionary[22],dictionary[23],dictionary[24],dictionary[25],dictionary[26],dictionary[27])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"7d561f9f-d190-450b-a301-b8ed6c2c6479","_uuid":"3c7b49cb0dafeac2b768e6ad7725e827bcaa6597","trusted":false},"cell_type":"code","source":"#scale it to all text\ncorpus = [dictionary.doc2bow(text) for text in all_text]\nend_corpus=time.time()\nprint(\"Time till corpus creation:\",end_clean-start_time,\"s\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"b9d4f99a-2465-4468-935f-a7d06b19b38b","_uuid":"1e18c21a9a9b48e7ab6fa62caa6c25853c17e2fb","trusted":false},"cell_type":"code","source":"#create the LDA model\nldamodel = LdaModel(corpus=corpus, num_topics=15, id2word=dictionary)\nend_lda=time.time()\nprint(\"Time till LDA model creation:\",end_lda-start_time,\"s\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"3994699d-f5ee-4c93-b1fe-80fcc895d2b8","_uuid":"97b4f8a12344135d026513afdde063cc0f229afc","trusted":false},"cell_type":"code","source":"pyLDAvis.enable_notebook()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"9d003ccd-bc78-4187-b5b0-efc8428db5ec","_uuid":"818f7538bf3ed10889afe7e24e5c7900a3f91f68","trusted":false},"cell_type":"code","source":"pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"4e325c77-ed84-46ec-bf92-cfe6146a1e55","_uuid":"ee455a40578533c794c4dc5da50a3ae97894636b","trusted":false},"cell_type":"code","source":"end_viz=time.time()\nprint(\"Time till viz:\",end_viz-start_time,\"s\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1f259757-05df-4647-b7aa-a4443d80b844","_uuid":"35ab11c74ca26a22ae8c8b9ad5edd8e0a23e655e"},"cell_type":"markdown","source":"**Chart Desc:** \n\nThe above visuals are from the awesome pyLDAviz package which is the python version of R package LDAviz.\n\nThe Left side shows the multi-dimensional \"word-space\" superimposed on two \"Principal components\" and the relative positions of all the topics.\n\nThe size of the circle represents what % of the corpus it contains.\n\nThe right side shows the word frequencies within the topic and in the whole corpus.\n\nClearly, some of the topics show a pattern of toxicity (ie) have a high contribution from toxic words.\n\nNow let's feed these topics into a model."},{"metadata":{"collapsed":true,"_cell_guid":"0c8597b1-c8bc-45ad-9049-47601c9d0677","_uuid":"03ee53b41821c2d1374008617ffb42eb7e1e0ee8","trusted":false},"cell_type":"code","source":"#creating the topic probability matrix \ntopic_probability_mat = ldamodel[corpus]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"cb53bffa-77d1-4c6f-8f6e-02129e51ac20","_uuid":"3c209e2ba4daebe6f37c5211178320d1b09644a9","trusted":false},"cell_type":"code","source":"#split it to test and train\ntrain_matrix=topic_probability_mat[:train.shape[0]]\ntest_matrix=topic_probability_mat[train.shape[0]:]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"5ee8279e-3727-4a25-9164-5f1093358cb1","_uuid":"b87246edb68e91758f88a3b535c5a3797b4b1ddd","trusted":false},"cell_type":"code","source":"del(topic_probability_mat)\ndel(corpus)\ndel(all_text)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"697efa7b-03b5-4878-adc4-5a8bdf9478b1","_uuid":"7d241d4dd9ce4944f6f96a75e69e5767e15a1173","trusted":false},"cell_type":"code","source":"#convert to sparse format (Csr matrix)\ntrain_sparse=gensim.matutils.corpus2csc(train_matrix)\ntest_sparse=gensim.matutils.corpus2csc(test_matrix)\nend_time=time.time()\nprint(\"total time till Sparse mat creation\",end_time-start_time,\"s\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"c8670207-d79a-431f-8b2a-33828b1ed955","_uuid":"940f7840af934eab2e581125861a773dbe7a418d","trusted":false},"cell_type":"code","source":"#custom NB model\nclass NbSvmClassifier(BaseEstimator, ClassifierMixin):\n    def __init__(self, C=1.0, dual=False, n_jobs=1):\n        self.C = C\n        self.dual = dual\n        self.n_jobs = n_jobs\n\n    def predict(self, x):\n        # Verify that model has been fit\n        check_is_fitted(self, ['_r', '_clf'])\n        return self._clf.predict(x.multiply(self._r))\n\n    def predict_proba(self, x):\n        # Verify that model has been fit\n        check_is_fitted(self, ['_r', '_clf'])\n        return self._clf.predict_proba(x.multiply(self._r))\n\n    def fit(self, x, y):\n        # Check that X and y have correct shape\n        y = y.values\n        x, y = check_X_y(x, y, accept_sparse=True)\n\n        def pr(x, y_i, y):\n            p = x[y==y_i].sum(0)\n            return (p+1) / ((y==y_i).sum()+1)\n\n        self._r = sparse.csr_matrix(np.log(pr(x,1,y) / pr(x,0,y)))\n        x_nb = x.multiply(self._r)\n        self._clf = LogisticRegression(C=self.C, dual=self.dual, n_jobs=self.n_jobs).fit(x_nb, y)\n        return self\n    \n\nmodel = NbSvmClassifier(C=2, dual=True, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"f62cc96a-9e99-4e9f-a2d9-df7cd79779d6","_uuid":"c5813ac756c6e4493cc2e539df38f7f07bc66d38","trusted":false},"cell_type":"code","source":"#set the target columns\ntarget_x=train_sparse.transpose()\nTARGET_COLS=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\ntarget_y=train[TARGET_COLS]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"66823cf7-864b-4c04-a359-a02f781b7281","_uuid":"a58ca24156285d0a5dd41c0ee95fc957fa97c197","trusted":false},"cell_type":"code","source":"del(train_sparse)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"021a1930-1a28-4bfc-bb8c-8b8ceabd8cbf","_uuid":"7e8dd0504f4a7bdce6a3af00a34139ab162cb3af","trusted":false},"cell_type":"code","source":"model = NbSvmClassifier(C=4, dual=True, n_jobs=-1)\nX_train, X_valid, y_train, y_valid = train_test_split(target_x, target_y, test_size=0.33, random_state=2018)\ntrain_loss = []\nvalid_loss = []\npreds_train = np.zeros((X_train.shape[0], y_train.shape[1]))\npreds_valid = np.zeros((X_valid.shape[0], y_train.shape[1]))\nfor i, j in enumerate(TARGET_COLS):\n    print('Class:= '+j)\n    model.fit(X_train,y_train[j])\n    preds_valid[:,i] = model.predict_proba(X_valid)[:,1]\n    preds_train[:,i] = model.predict_proba(X_train)[:,1]\n    train_loss_class=log_loss(y_train[j],preds_train[:,i])\n    valid_loss_class=log_loss(y_valid[j],preds_valid[:,i])\n    print('Trainloss=log loss:', train_loss_class)\n    print('Validloss=log loss:', valid_loss_class)\n    train_loss.append(train_loss_class)\n    valid_loss.append(valid_loss_class)\nprint('mean column-wise log loss:Train dataset', np.mean(train_loss))\nprint('mean column-wise log loss:Validation dataset', np.mean(valid_loss))\n\n\nend_time=time.time()\nprint(\"total time till NB base model creation\",end_time-start_time)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"c3e029a1-bf08-48a4-a24b-0b8ded04b270","_uuid":"6ae92f3bee44302930042f00919b86ea728e9198","trusted":false},"cell_type":"code","source":"#credits\n#pyLDAviz\n#https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf\n\n#to be continued \n#to do next\n#paragraph vectors\n#https://arxiv.org/abs/1507.07998\n#https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"baa80b0e-9fe1-42cf-8920-1277e6666cb7","_uuid":"af17ef8a70169c4cb1e57f6e026e12fc14a10878","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"281daa9d-5c39-4096-bb4a-9024137b55f2","_uuid":"97e49a5c01d26c859cf257eb9e6791c8740919fc","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}