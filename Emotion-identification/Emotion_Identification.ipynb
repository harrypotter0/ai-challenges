{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.models import load_model\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils import plot_model\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path to folders/files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_FOLDER = '../AI_data/'\n",
    "PATH_TO_Glove_vectors = '../glove.twitter.27B/glove.twitter.27B.200d.txt'\n",
    "PATH_TO_Word2Vec_vectors = '../glove.twitter.27B/glove.twitter.27B.200d.word2vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(data_frame['emotion'].values)\n",
    "emotion_labels = {'anger':0, 'happiness':1, 'love':2, 'neutral':3, 'sadness':4, 'worry': 5}\n",
    "emotion_labels_reverse = {0:'anger', 1:'happiness', 2:'love', 3:'neutral', 4:'sadness', 5:'worry'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_glove_to_word2vec_vectors(PATH_TO_Glove_vectors, PATH_TO_Word2Vec_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = read_word2vec_vectors(PATH_TO_Word2Vec_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename, delimiter):\n",
    "    data = pd.read_csv(filename, delimiter='\\t')\n",
    "    data = data.drop('Unnamed: 0', axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_glove_to_word2vec_vectors(filepath, output_filepath):\n",
    "    glove2word2vec(filepath, output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_word2vec_vectors(filepath):\n",
    "    model = KeyedVectors.load_word2vec_format(filepath, binary=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    tokens = [word for word in stripped if word.isalpha()]\n",
    "    stop_words = stopwords.words('english')\n",
    "    token_list = []\n",
    "    for word in tokens:\n",
    "        if not word in stop_words:\n",
    "            token_list.append(word)\n",
    "    #token_list = [TextBlob(word).correct() for word in token_list]\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(data_frame):\n",
    "    train_data = []\n",
    "    for review in data_frame['review'].values:\n",
    "        review = preprocess(review)\n",
    "        train_data.append(review)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_into_categorical(data_frame):\n",
    "    categorical_list = []\n",
    "    for emotion in data_frame['emotion'].values:\n",
    "        categorical_list.append(emotion_labels[emotion])\n",
    "    categorical_list = np_utils.to_categorical(categorical_list)\n",
    "    return categorical_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_words(data):\n",
    "    unique_words = []\n",
    "    for l in data:\n",
    "        unique_words += l\n",
    "    unique_words = list(set(unique_words))\n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_words_into_numbers(data, unique_words):\n",
    "    data_in_numbers = []\n",
    "    for l in data:\n",
    "        data_in_numbers.append([unique_words.index(word) for word in l])\n",
    "    return data_in_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_train_data = PATH_TO_FOLDER + 'emotion_trainingdataset.csv'\n",
    "train_data = read_data(filename_train_data, '\\t')\n",
    "\n",
    "filename_test_data = PATH_TO_FOLDER + 'emotion_testdataset.csv'\n",
    "test_data = read_data(filename_test_data, '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocess_df(train_data)\n",
    "X_test = preprocess_df(test_data)\n",
    "unique_words = unique_words(X_train + X_test)\n",
    "X_train_set = convert_words_into_numbers(X_train, unique_words)\n",
    "X_test = convert_words_into_numbers(X_test, unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_set = convert_labels_into_categorical(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport random\\nnum = list(range(len(X_train_set)))\\nprint(num)\\nrandom.shuffle(num)\\nprint(num)\\nX_train_set = [X_train_set[n] for n in num]\\ny_train_set = [y_train_set[n] for n in num]\\n'"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import random\n",
    "num = list(range(len(X_train_set)))\n",
    "print(num)\n",
    "random.shuffle(num)\n",
    "print(num)\n",
    "X_train_set = [X_train_set[n] for n in num]\n",
    "y_train_set = [y_train_set[n] for n in num]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(X_train_set)\n",
    "point = int(length * 0.75)\n",
    "X_train = X_train_set[:point]\n",
    "y_train = y_train_set[:point]\n",
    "X_validation = X_train_set[point:]\n",
    "y_validation = y_train_set[point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_28 (Embedding)     (None, 200, 32)           235776    \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 200, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_41 (LSTM)               (None, 20)                4240      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 6)                 126       \n",
      "=================================================================\n",
      "Total params: 240,142\n",
      "Trainable params: 240,142\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "number_of_words = len(unique_words)\n",
    "max_length_of_input = 200\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, max_length_of_input)\n",
    "X_test = sequence.pad_sequences(X_test, max_length_of_input)\n",
    "X_validation = sequence.pad_sequences(X_validation, max_length_of_input)\n",
    "\n",
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(number_of_words, embedding_vector_length, input_length = max_length_of_input))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(LSTM(20, return_sequences=True))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(LSTM(20))\n",
    "model.add(Dense(6, activation = 'sigmoid'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "#plot_model(model, to_file='model_LSTM.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 200\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train[0]), len(X_validation[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(X_validation).shape)\n",
    "print(np.array(y_validation).shape)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_train, accuracy_train = model.evaluate(X_train, y_train)\n",
    "print(score_train, accuracy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../AI_models/5_model_LSTM' + '_' + str(30) + '_' + str(accuracy_train) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(test_sentence, model):\n",
    "    test_sentence = preprocess(test_sentence)\n",
    "    test_sentence = [unique_words.index(word) for word in test_sentence]\n",
    "    print(test_sentence)\n",
    "    test_sentence = sequence.pad_sequences([test_sentence], max_length_of_input)\n",
    "    print(model.predict(test_sentence))\n",
    "    predicted_class = model.predict_classes(test_sentence)[0]\n",
    "    emotion = emotion_labels_reverse[predicted_class]\n",
    "    print(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2567, 4252, 2516, 1520, 2308, 5558, 4966, 5786, 1416, 5406, 6791]\n",
      "[[2.5340414e-04 4.2796298e-04 4.4181332e-04 6.4654440e-01 2.0610722e-04\n",
      "  6.8068277e-04]]\n",
      "neutral\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"WOW i just drank a drink of water - 12 ice cubes that took ages to melt. i now have brian freeze\"\n",
    "predict_class(test_sentence, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
