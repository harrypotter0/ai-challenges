{"cells":[{"metadata":{"_cell_guid":"b627d459-3a31-476b-b91a-a247fad1e210","_uuid":"bed9e689ce761fa618cb23bea7990112dd8db48f"},"cell_type":"markdown","source":"Here, we are going to look at dimensionality reduction as a preprocessing technique for images.\n\nBefore we start, why might you do this? Well the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) tells us that the more features we have then the more data we need to train a good model. Expanding on this, if you have a fixed amount of training data (which is often the case) your model's accuracy will decrease for every feature you have.\n\nFor images, we think of the number of features as the number of pixels. So for a 64x64 image we have 4096 features! One way to reduce that number (and hopefully produce a more accurate model) is to effectively compress the image. We do this by trying to find a way of keeping as much information as possible about the image without losing the essential structure.\n\nFor the example in this notebook, we're going to use [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) and the Sign Language Digits classification dataset."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":false,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.decomposition import PCA\n\nX = np.load('../input/Sign-language-digits-dataset/X.npy')\nY = np.load('../input/Sign-language-digits-dataset/Y.npy')\n\nX.shape","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"8f3f0440-8061-4065-8e2a-30b627fd22b5","_uuid":"4229a805cffda56a9feca2a0f811d1ec0d188f1d"},"cell_type":"markdown","source":"So as I said before the Sign-language-digits-dataset is formed of 2062 images, each 64x64.\n\nLet's have a look at what that looks like (this is the number 9):"},{"metadata":{"_cell_guid":"05b9e856-590d-4816-83fa-0dadf2b480bc","_uuid":"e1ef2ef125d5bd07fa9118548e1a51d4b0893ecf","trusted":true},"cell_type":"code","source":"plt.imshow(X[0])","execution_count":49,"outputs":[]},{"metadata":{"_cell_guid":"eeab07bb-6f1c-4d15-bf8e-45e722cf4728","_uuid":"c56fb1d4b1618e9064a620706f5609df45e1b7d4"},"cell_type":"markdown","source":"To start with let's flatten our data into 2062 4096 dim vectors and split the dataset into training and testing sets."},{"metadata":{"_cell_guid":"78a459c8-fdee-4e4a-9b93-fe1326b809e6","_uuid":"d57663d040803e65343b118d821ca52270f0472d","collapsed":true,"trusted":true},"cell_type":"code","source":"X_flat = np.array(X).reshape((2062, 64*64))\n\nX_train, X_test, y_train, y_test = train_test_split(X_flat, Y, test_size=0.3, random_state=42)","execution_count":50,"outputs":[]},{"metadata":{"_uuid":"d69c672171f1f983b217cea767e60f9b48fffc75"},"cell_type":"markdown","source":"To demonstrate how dimensionality reduction can improve the results of a model we need a model. Here is a very basic, fully connected neural net\n\nThis is deliberately not a great model and I'm not going to tune the hyper-parameters. We only need this as a benchmark for later"},{"metadata":{"_cell_guid":"3c3026fe-2b32-441b-8353-ef26727773fb","_uuid":"2778c277e319770be5a55e877f1a706b7e9cb1a9","trusted":true},"cell_type":"code","source":"clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(100, 100, 100, 100), random_state=1)\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3c2af9ac-aff5-4124-bbb2-f8154efe1122","_uuid":"3e7153ee2143381bb424f5cb1bac96dfaa30983f","trusted":true},"cell_type":"code","source":"y_hat = clf.predict(X_test)\n\nprint(\"accuracy: \" + str(accuracy_score(y_test, y_hat)))","execution_count":52,"outputs":[]},{"metadata":{"_uuid":"e5695b33371917fa50552e944254aa0b103d73aa"},"cell_type":"markdown","source":"As you can see, this is a pretty poor model, only achieving ~47% overall accuracy on the test set.\n\nWe're now goint to reduce the dimension of our training data and then retrain what we have.\n\nThe objective here is going to be to reduce the number of dimensions of the image, but before we do that we need to decide what we want to reduce it to. To do that we're going to try and find the number of dimensions that keeps 95% of the variance of the original images."},{"metadata":{"_cell_guid":"8bddb62c-6b6c-43cd-b309-cd036a5cfd89","_uuid":"2fe372d88145995f55071ee07eb893aef6831388","collapsed":true,"trusted":true},"cell_type":"code","source":"pca_dims = PCA()\npca_dims.fit(X_train)\ncumsum = np.cumsum(pca_dims.explained_variance_ratio_)\nd = np.argmax(cumsum >= 0.95) + 1","execution_count":53,"outputs":[]},{"metadata":{"_cell_guid":"630eb3f7-7296-46a8-ab63-8ba152668229","_uuid":"bce1f8280f424ac5989f9454e101a4f2d09428e2","trusted":true},"cell_type":"code","source":"d","execution_count":54,"outputs":[]},{"metadata":{"_uuid":"6fbf8e29216cec77daf6b9e9151846969dabb510"},"cell_type":"markdown","source":"Wow - so we've gone from 4096 dimensions to just 292! But how good is this actually?\n\nLet's train PCA on our training set and transform the data, then print out an example"},{"metadata":{"_cell_guid":"781a02d0-b04a-4d36-ac2c-7b65f5a5976f","_uuid":"97aba0abb36ea73b26ea074b9a82a665cfe99bfd","collapsed":true,"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=d)\nX_reduced = pca.fit_transform(X_train)\nX_recovered = pca.inverse_transform(X_reduced)","execution_count":55,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a33298392a91ba8cf2dae70fb6c9e541f988f1c"},"cell_type":"code","source":"print(\"reduced shape: \" + str(X_reduced.shape))\nprint(\"recovered shape: \" + str(X_recovered.shape))","execution_count":56,"outputs":[]},{"metadata":{"_cell_guid":"99a4082c-2686-46cc-8058-a4f322246639","_uuid":"e4c9ce9bda7df7dcfcc83d791b2e49f170d65894","trusted":true},"cell_type":"code","source":"f = plt.figure()\nf.add_subplot(1,2, 1)\nplt.title(\"original\")\nplt.imshow(X_train[0].reshape((64,64)))\nf.add_subplot(1,2, 2)\n\nplt.title(\"PCA compressed\")\nplt.imshow(X_recovered[0].reshape((64,64)))\nplt.show(block=True)","execution_count":57,"outputs":[]},{"metadata":{"_uuid":"1ef95e0e0e7b974aaf58bac33dd95f72819c2201"},"cell_type":"markdown","source":"You can see it's far from perfect, but it's still clear what shape the hand is making\n\nLet's retrain our model with the dimensionally reduced training data:"},{"metadata":{"trusted":true,"_uuid":"26a2c4b4b9660d957ea34668488bd8d90796cf96"},"cell_type":"code","source":"clf_reduced = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(100, 100, 100, 100), random_state=1)\nclf_reduced.fit(X_reduced, y_train)","execution_count":58,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45d9e9c6147d442c3c31c058bdfdb04961a06598"},"cell_type":"code","source":"X_test_reduced = pca.transform(X_test)\n\ny_hat_reduced = clf_reduced.predict(X_test_reduced)\n\nprint(\"accuracy: \" + str(accuracy_score(y_test, y_hat_reduced)))","execution_count":59,"outputs":[]},{"metadata":{"_uuid":"4d58d624be1f0b8bacca5d06d38e6af911e25c0e"},"cell_type":"markdown","source":"And as you can see we've taken this simple model from ~47% accuracy on the test set to over 70%"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"a7bb44311b2b42cf90553bb4647011fd91192dfb"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}