{"cells":[{"metadata":{"_uuid":"a20f3f412c6e58acfda930c402cd48a7b27ad782"},"cell_type":"markdown","source":"**Hello Everyone**\n<br/>\nI have been using keras and tensorflow for an year and both have been good with few advantages and limitations.\n<br/>\nKeras is high level API built on Theano and tensorflow and is easy to use while tensorflow is a complex one.But in flexibility point of view, TF in much more flexible and offers more advanced operations than keras which comes useful while doing research.\n*So I then decided to find something which is easy to use, provides the intuition of entire flow control and offers flexibilty of doing advanced operations.*This is where I started to work and fall in love with it.\nFor the crowd who haven't heard much of it;\n<br/>\n**PyTorch is an Open source Machine Learning library for python, based on Torch.** \n* **The biggest difference between TF and PYtorch is that, in tensorflow, graph is defined statically before running a model whereas in PyTorch, graph construction is dynamic and nodes can be defined, changed and executed on the go.**\n* **Building or binding custom extensions written in C, C++ or CUDA is doable with both frameworks. TensorFlow again requires more boiler plate code though is arguably cleaner for supporting multiple types and devices. In PyTorch you simply write an interface and corresponding implementation for each of the CPU and GPU versions.**\n\n* **Compiling the extension is also straight-forward with both frameworks and doesn’t require downloading any headers or source code outside of what’s included with the pip installation.**\n\nThis tutorial focusses on implementing Image classification task using PyTorch using Sequential API. The difference between torch.nn and torch.nn.functional is a matter of convenience and taste. *torch.nn is more convenient for methods which have learnable parameters.*\n<br/>\n**This tutorial is sufficient to create your first model in PyTorch and learn most of it's concepts and this documentation is very straightforward and descriptive.**\n<br/>\nInitially, I have noticed people face issues in these issues with pytorch:\n* **Data Loading**\n* **Model Evaluation**\n<br/>\nso I have intended to make these tasks more simpler in this tutorial. This tutorial is sufficient for building your first classifier using PyTorch and optimizing it using your custom data. It is structured in such way that you can get hold of entire pipeline of Image Classification tasks. Just Fork the kernel and execute it sequentially and you will get practical taste of PyTorch.\n\nTraining a classifier involves these steps:\n* **Data Loading and Preprocessing**\n* **Model Selection**\n* **Defining a loss function**\n* **Training the classifier, validation and Parameter tuning**\n<br/>\n\nWe are now set for a ride in all these using PyTorch. I have invested my research methodology while designing this tutorial so that any beginner can easily understand.\n<br/>\n    Let's begin by importing the required libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch.nn.functional as F\nimport math\nfrom torch.optim import lr_scheduler\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nimport torch\nimport itertools\nfrom torchvision import models\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nfrom torch.nn import MaxPool2d\nimport chainer.links as L\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nplt.ion()","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"498e397d70c25891668e9307d0150117e8023008"},"cell_type":"markdown","source":"**This data is wrongly matched. Please execute this code to have the correct mapping of X and y values**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"49053add31b5d54b909cfe0127576b1545cce1a8"},"cell_type":"code","source":"data = np.load('../input/Sign-language-digits-dataset/X.npy')\ntarget = np.load('../input/Sign-language-digits-dataset/Y.npy')\nY = np.zeros(data.shape[0])\nY[:204] = 9\nY[204:409] = 0\nY[409:615] = 7\nY[615:822] = 6\nY[822:1028] = 1\nY[1028:1236] = 8\nY[1236:1443] = 4\nY[1443:1649] = 3\nY[1649:1855] = 2\nY[1855:] = 5\nX_train, X_test, y_train, y_test = train_test_split(data, Y, test_size = .02, random_state = 2) ## splitting into train and test set","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"6582763d0e4660970e73a6232579fa973a53c528"},"cell_type":"markdown","source":"# Custom Data Loading and Processing"},{"metadata":{"_uuid":"30ad8142fc3b625b5d0c0cb6ea8623882e30d12b"},"cell_type":"markdown","source":"Here we are working with numpy arrays which can be directly converted to torch variable for mathematical operations.Many datasets contain images which can be imported using PIL library using\n<br/>\n**PIL.IMAGE..fromarray(img_name)**\n<br/>\nWe use PIL instead of OpenCV because its Torch default image loader and is compatible with ToTensor() method. The PyTorch website has all the tutorials of this section using their inbuild Dataset class and it can be a toilsome task to load data in case of Custom Dataset.While loading the data, these point should be closely worked upon:\n* Setting a batch Size\n* Shuffling the data\n* Parallelizing the tasks using multiprocecssing workers.\n<br/>\n\nThe **DataLoader** function  provides all of these features and you can specify them before heading to next task. The label or the target variable can be one an array or one-hot encoded depending on the loss function. For having a look at the different loss functions, please head to [Pytorch loss functions](https://pytorch.org/docs/master/nn.html#loss-functions)"},{"metadata":{"_uuid":"e6ae607c026840fba9a43031a685bad1b4348094"},"cell_type":"markdown","source":"Data loading in PyTorch can be separated in 2 parts:\n<br/>\n* Data must be wrapped on a Dataset parent class where the methods __getitem__ and __len__ must be overrided. Not that,  the data is not loaded on memory by now.\n* The Dataloader reads the data and puts it into memory."},{"metadata":{"_uuid":"1418342011d9084f4f6ef1d700a3d48997d0975e"},"cell_type":"markdown","source":"### torchvision\nIt is used to load and prepare dataset. Using it you can create *transformations* on the input data.\n<br/>\n#### transforms\nIt is used for preprocessing images and performing operations sequentially.\n<br/>\n#### num_workers\nIt is used for multiprocessing.Normally, **num_workers = 4 * (number of gpus)** works well.\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"064e7c6c78d740334f28d1ca5ce6cd866efefc90"},"cell_type":"code","source":"class DatasetProcessing(Dataset):\n    def __init__(self, data, target, transform=None): #used to initialise the class variables - transform, data, target\n        self.transform = transform\n        self.data = data.reshape((-1,64,64)).astype(np.uint8)[:,:,:,None]\n        self.target = torch.from_numpy(target).long() # needs to be in torch.LongTensor dtype\n    def __getitem__(self, index): #used to retrieve the X and y index value and return it\n        return self.transform(self.data[index]), self.target[index]\n    def __len__(self): #returns the length of the data\n        return len(list(self.data))","execution_count":29,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45c462dc91c59fac23237ec90c26736892f994f1","collapsed":true},"cell_type":"code","source":"transform = transforms.Compose(\n    [transforms.ToPILImage(), transforms.ToTensor(), transforms.Normalize(mean=(0.5,), std=(0.5,))])\ndset_train = DatasetProcessing(X_train, y_train, transform)\ntrain_loader = torch.utils.data.DataLoader(dset_train, batch_size=4,\n                                          shuffle=True, num_workers=4)","execution_count":30,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96fad45374e7af8c9afdc5146203f9906630a444","collapsed":true},"cell_type":"code","source":"dset_test = DatasetProcessing(X_test, y_test, transform)\ntest_loader = torch.utils.data.DataLoader(dset_test, batch_size=4,\n                                          shuffle=True, num_workers=0)","execution_count":31,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39298efffccec09f281900748b7606dfb6869bfb"},"cell_type":"code","source":"for num, x in enumerate(X_train[0:6]):\n    plt.subplot(1,6,num+1)\n    plt.axis('off')\n    plt.imshow(x)\n    plt.title(y_train[num])","execution_count":32,"outputs":[]},{"metadata":{"_uuid":"5ac9fda2e27d63336b650e3fc97afbb87e6bd85e"},"cell_type":"markdown","source":"# Training\n\n## Define a CNN\n\n![](https://www.researchgate.net/publication/318174473/figure/fig2/AS:614173574701099@1523441799965/The-topology-of-the-event-driven-Convnet-used-for-this-work-Where-n-is-the-size-of-the.png)\n\n\n\n\n\n\n\n\n"},{"metadata":{"_uuid":"0dc8f5234ce192877d6ac3d939147c405c8bb21f"},"cell_type":"markdown","source":"Some of the important libraries\n### [nn](https://pytorch.org/docs/master/nn.html)\nThis library contains  function for building conv nets and importing a loss function.\nSome loss functions are:\n* **binary_cross_entropy**: Function that measures the Binary Cross Entropy between the target and the output.\n* **nll_loss**: measures the The negative log likelihood loss.\n* **cross_entropy**: This criterion combines log_softmax and nll_loss in a single function.\n\n### [optim](https://pytorch.org/docs/master/optim.html)\ntorch.optim is a package implementing various optimization algorithms. To construct an Optimizer you have to give it an iterable containing the parameters (all should be Variable s) to optimize. Then, you can specify optimizer-specific options such as the learning rate, weight decay, etc.Some of the optimizers defined by this library are:\n* **Adam**: It has been proposed in [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)\n* **LBFGS**: Implements L-BFGS algorithm.\n* **RMSprop**: Proposed by G. Hinton in his [course](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n* **SGD**: Nesterov momentum is based on the formula from [On the importance of initialization and momentum in deep learning]\n<br/>\n\n\nLet’s use a **Classification Cross-Entropy loss** and **SGD with momentum**."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ff966018137010633751cab2ca6566dce1059678"},"cell_type":"code","source":"class Net(nn.Module):    \n    def __init__(self):\n        super(Net, self).__init__()\n          \n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n          \n        self.classifier = nn.Sequential(\n            nn.Dropout(p = 0.5),\n            nn.Linear(32 * 32 * 32, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p = 0.5),\n            nn.Linear(512, 10),\n        )\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        \n        return x     ","execution_count":33,"outputs":[]},{"metadata":{"_uuid":"f6240d8e37d0dd210d4f208b3e442e925e56cd53"},"cell_type":"markdown","source":"# How to use optimizers?\nTo construct an Optimizer you have to give it an iterable containing the parameters (all should be Variable s) to optimize. Then, you can specify optimizer-specific options such as the learning rate, weight decay, etc.<br/>\n*If you want to move the model and variables to cuda, use **model_object.cuda()** and **variable_name.cuda()***\n\n**Follow these steps to build your first model in pytorch**\n\n#### Clearing the Gradients - optimizer.zero_grad()\n** Clear out the gradients accumulated for the parameters of the network before calling loss.backward() and optimizer.step()**\n#### Compute the loss - criterion( predicted_target, target)\n**Compute the loss between the predicted value and the target value within the loss function previously defined**\n#### Backpropogation - loss.backward()\n**Back-prop all the layers in all the layers of the network**\n#### Taking an optimization step - optimizer.step()\n** Update the parameters of the network **\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"16f037ec9b1bf98ae0d76a9858d1e6a88be7ee12"},"cell_type":"code","source":"model = Net()\n\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\ncriterion = nn.CrossEntropyLoss()\n\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\nif torch.cuda.is_available():\n    model = model.cuda()\n    criterion = criterion.cuda()","execution_count":34,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"07c17742b73c3c5ec68c6d98588fd568d97e557f"},"cell_type":"code","source":"def train(epoch):\n    model.train()\n    exp_lr_scheduler.step()\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = Variable(data), Variable(target)\n        #print(data.size())\n        if torch.cuda.is_available():\n            data = data.cuda()\n            target = target.cuda()\n        #print(target)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        \n        loss.backward()\n        optimizer.step()\n        \n        if (batch_idx + 1)% 100 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n                100. * (batch_idx + 1) / len(train_loader), loss.data[0]))","execution_count":35,"outputs":[]},{"metadata":{"_uuid":"7b522dea57be5ea5ec35de8be06e8bdd40fba17d"},"cell_type":"markdown","source":"## Model Evaluation\nNow, it's time to evaluate out model. <br/>\n**Note that I have used Functional API( F.cross_entropy( args )) so as to give an idea how it works.**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0dfb2905ae9e3b2dbcbb82fca118a28968b683d3"},"cell_type":"code","source":"def evaluate(data_loader):\n    model.eval()\n    loss = 0\n    correct = 0\n    \n    for data, target in data_loader:\n        data, target = Variable(data, volatile=True), Variable(target)\n        if torch.cuda.is_available():\n            data = data.cuda()\n            target = target.cuda()\n        \n        output = model(data)\n        loss += F.cross_entropy(output, target, size_average=False).data[0]\n        pred = output.data.max(1, keepdim=True)[1]\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n        \n    loss /= len(data_loader.dataset)\n        \n    print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n        loss, correct, len(data_loader.dataset),\n        100. * correct / len(data_loader.dataset)))","execution_count":36,"outputs":[]},{"metadata":{"_uuid":"4cbea2b5e32272e2f591c1c430f79a95e28e8bf4"},"cell_type":"markdown","source":"### You can run this model for more  epochs"},{"metadata":{"trusted":true,"_uuid":"fd6e50c515a60427956abef603e35669aea627d7"},"cell_type":"code","source":"n_epochs = 2\n\nfor epoch in range(n_epochs):\n    train(epoch)\n    evaluate(train_loader)","execution_count":37,"outputs":[]},{"metadata":{"_uuid":"9c03d543be006fa8e7961339f6da817af581ea20"},"cell_type":"markdown","source":"**One powerful technique to increase the  performance of any deep learning model is through adding more data. So Let's use data augmentationn and see if we get some performance improvement.**"},{"metadata":{"_uuid":"61c236cbf3a487485178d446489dabcf9d309e1a"},"cell_type":"markdown","source":"## Data Augmentation"},{"metadata":{"trusted":true,"_uuid":"0050ff4cf7c2fa816a555522fb28e27f1397fbd1"},"cell_type":"code","source":"train_transform= transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.RandomHorizontalFlip(), # Horizontal Flip\n            transforms.Scale(256), #Scaling the input\n            transforms.RandomCrop(64, padding=4), # Centre Crop\n            transforms.ToTensor(),  #Convereting the input to tensor\n            transforms.Normalize([0.5], [0.5])\n])\ndset_train = DatasetProcessing(X_train, y_train, train_transform)\ntrain_loader = torch.utils.data.DataLoader(dset_train, batch_size=4,\n                                          shuffle=True, num_workers=4)","execution_count":38,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f2ccaf0bc43988b210cbea835c01771dc78088d"},"cell_type":"code","source":"n_epochs = 5\n\nfor epoch in range(n_epochs):\n    train(epoch)","execution_count":39,"outputs":[]},{"metadata":{"_uuid":"427d583b14b17be1eb09681c64eb8a4a761c91ff"},"cell_type":"markdown","source":"\n**Please give your feedback on this script and Upvote if you liked it.**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}